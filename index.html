<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoNRG: Egocentric Navigation Gesture Dataset">
  <meta name="keywords" content="EgoNRG, Egocentric Navigation Gesture Dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoNRG: Egocentric Navigation Gesture Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>
-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EgoNRG</h1>
          <h2 class="title is-2 publication-title">Egocentric Gesture Dataset for Robust Human-Robot Communication via Head
            Mounted Devices in Industrial and Military Settings</h2>
          <div class ="column is-full_width">
            <h2 class="title is-4">
              Winter Conference on Applications of Computer Vision (
              <a href="https://wacv.thecvf.com/">WACV</a>
              ) 2026
            </h2>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://frankregal.com">Author 1</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="sanatnair@utexas.edu">Author 2</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Author 3</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://robotics.me.utexas.edu/">Author 4</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institution Hidden for Blind Review</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/DC4J0Q"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-circle-down"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UTNuclearRobotics/EgoNRG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
          <div class ="column is-full_width">
            <h3 class="title is-4">
              <i>Submission is currently under review.</i>
            </h3>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">EgoNRG: Egocentric Navigation Robot Gesture Dataset</span>
      </h2>
    </div>
  </div>
</section> -->




<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Gesture recognition in Human-Robot Interaction (HRI) presents unique challenges 
            in industrial, military, and emergency response settings where operators must 
            communicate non-verbal navigation commands to semi-autonomous robotic systems 
            while wearing Personal Protective Equipment (PPE). We present EgoNRG (Egocentric 
            Navigation Robot Gestures), a comprehensive dataset comprising 3,000 first-
            person multi-view video recordings and 160,000 annotated images captured from 
            four strategically positioned head-mounted cameras. The dataset addresses 
            critical limitations in existing egocentric gesture recognition systems by 
            providing joint hand-arm segmentation labels for both covered and uncovered 
            limbs across diverse environmental conditions. Our multi-viewpoint collection 
            methodology captures twelve navigation gestures (ten Army Field Manual-derived, 
            one deictic, and one emblem gestures) performed by 32 participants across indoor 
            and outdoor environments with systematic variations in clothing conditions and 
            background complexity. Comprehensive experiments demonstrate the dataset's 
            effectiveness for training robust gesture recognition systems suitable for real-
            world deployment in challenging operational environments.
          </p>
        </div>
      </div>
    </div>
    <!-- Paper image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-image">
          <img src="./static/images/dataset_overview.png" alt="EgoNRG Dataset Overview" style="width: 100%; max-width: 800px;">
        </div>
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            Overview of the EgoNRG dataset contents. The dataset contains 3,044 videos and 160,639 annotated frames captured from 
            the egocentric perspective. 32 participants performed 12 gestures related to ground vehicle robot control. The dataset features
            conditions with and without background people visible, and both indoor and outdoor environments. The dataset was also recorded from
            four synchronized monochrome cameras each with a different perspective of each gesture performed by the participants.
          </p>
        </div>
      </div>
    </div>
    <!--/ Paper image. -->
    <!--/ Abstract. !--> 
    <!-- Paper video.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!-/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Dataset Overview</h2>
          <p>
            EgoNRG -Egocentric Navigation Robot Gestures- is a comprehensive dataset features joint hand and arm segmentations captured from 32 participants 
            (14 females and 18 males) performing 12 gesture-based commands for ground vehicle robot control. The 
            participants were divided into four groups of eight, with each group executing a specific set of 
            four gestures. Ten of the twelve gestures were derived from the Army Field Manual, 1 deictic gesture, and 1 emblem gesture. The dataset encompasses 3,044 videos and 160,639 annotated frames. The dataset features:
          </p>
          <ul> 
            <li>Joint hand and arm segmentations of each participants' left and right limb.</li>
            <li>Participants' performed gestures with 1. long sleeves and gloves (wearing replica flame-resistant solid color clothing and military camouflage) and 
            2. bare skin to mimic conditions in real-world industrial and military environments.</li>
            <li>Environments with and without background people visible.</li>
            <li>Data captured in both indoor and outdoor environment at various points throughout the day (morning, midday, and dusk).</li>
            <li>Data captured from four synchronized monochrome cameras each with a different perspective.</li>
            <li>Gesture performed map directly to standard ground vehicle robot commands (stop, move forward, go left, move in reverse, etc.).</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Section Title -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Segmentation Examples</h2>

        </div>
      </div> 
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-image">
          <img src="./static/images/dataset_masks_subset.png" alt="EgoNRG Dataset Gestures" style="width: 100%;">
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
        <p style="margin-top: 20px">
          These are examples of the segmentation masks that were annotated for the EgoNRG dataset.
          You can seee the segmentation masks created for the joint hand-arm for both the left and right limbs.
          You can also see how the dataset has varying clothing conditions, light conditions, and background people visible.
        </p>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/_gesture_example_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Third person point of view of the gestures captured from the first-person (egocentric) view in the EgoNRG dataset.</span>
      </h2>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Gesture Classes</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-image">
                <img src="./static/images/gestures_overview_static.png" alt="EgoNRG Dataset Gestures" style="width: 100%;">
              </div>
            </div>
          </div>
          <p style="margin-top: 30px">
            Above is an image depicting a static representation of the 12 gestures performed in the dataset. 10 where adopted from the Army Field Manual, and 
            1 is a deictic gesture "point", and 1 emblem gesture "approve". Below is a video recorded in the third-person view showing an example of each gesture 
            class that was captured in the dataset from the first-person view.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/gif_gesture_examples.mp4"
                    type="video/mp4">
          </video>
          <p style="margin-top: 0px">
            Above is a video showing the 12 gestures being performed from the third-person viewpoint. This is just for reference.
            All gestures in the dataset were captured using the first-person point of view.
          </p>
        </div>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Example Videos</h2>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/example_videos_a_cropped.mp4"
                    type="video/mp4">
          </video>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/example_videos_b_cropped.mp4"
                    type="video/mp4">
          </video>
          <p style="margin-top: 0px">
            Examples of raw videos captured from the various viewpoints of the cameras. You can see gestures being performed
            in both indoor and outdoor environments, with participants wearing long sleeves and gloves as well as bare skin,
            in varying lighting conditions, with and without background people visible.
          </p>
        </div>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Example Viewpoints</h2>
          <div class="columns is-centered has-text-centered">
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/combined_video_example_cropped.mp4"
                      type="video/mp4">
            </video>
            <p style="margin-top: 0px">
              This video shows all four viewpoints that were captured in the dataset for each gesture performed by the participants.
              You can see that in each viewpoint, the part of the participants hand that is visible is different across viewpoints,
              hence providing more information for training models that can generalize to other egocentric vision platforms.
            </p>
        </div>
      </div> 
    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> 
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> 
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> 
-->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{last2026egocentric,
  author    = {last, fist and last, first and last, first and last, first},
  title     = {Egocentric Gesture Dataset for Robust Human-Robot Communication via Head Mounted Devices in Industrial and Military Settings},
  journal   = {IEEE/CVF Winter Conference on Applications of Computer Vision},
  year      = {SUBMITTED - UNDER REVIEW},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            Special thanks to <a href="https://github.com/keunhong">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
