<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoNRG: Egocentric Navigation Gesture Dataset">
  <meta name="keywords" content="EgoNRG, Egocentric Navigation Gesture Dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoNRG: Egocentric Navigation Robot Gesture Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>
-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EgoNRG</h1>
          <h2 class="title is-2 publication-title"> Egocentric Navigation Robot Gesture Dataset</h2>
          <div class ="column is-full_width">
            <h2 class="title is-5 publication-title"> Egocentric Gesture Dataset for Robust Human-Robot Communication via Head
            Mounted Devices in Industrial and Military Settings</h2>
            <h2 class="title is-5">
              20th IEEE International Conference on Automatic Face and Gesture Recognition
            </h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://frankregal.com">Frank Regal</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="sanatnair@utexas.edu">Sanat Nair</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Asha Karmakar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://robotics.me.utexas.edu/">Mitch Pryor</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IThe University of Texas at Austin</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/DC4J0Q"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid fa-circle-down"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UTNuclearRobotics/EgoNRG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
          <div class ="column is-full_width">
            <h3 class="title is-4">
              <i>Submission to <a href="https://fg2026.ieee-biometrics.org/">FG 2026</a> is currently in review.</i>
            </h3>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Gesture recognition in Human-Robot Interaction (HRI) presents unique challenges 
            in industrial, military, and emergency response settings where operators must 
            communicate non-verbal navigation commands to semi-autonomous robotic systems 
            while wearing Personal Protective Equipment (PPE). We present EgoNRG (Egocentric 
            Navigation Robot Gestures), a comprehensive dataset comprising 3,000 first-
            person multi-view video recordings and 160,000 annotated images captured from 
            four strategically positioned head-mounted cameras. The dataset addresses 
            critical limitations in existing egocentric gesture recognition systems by 
            providing joint hand-arm segmentation labels for both covered and uncovered 
            limbs across diverse environmental conditions. Our multi-viewpoint collection 
            methodology captures twelve navigation gestures (ten Army Field Manual-derived, 
            one deictic, and one emblem gestures) performed by 32 participants across indoor 
            and outdoor environments with systematic variations in clothing conditions and 
            background complexity. Comprehensive experiments demonstrate the dataset's 
            effectiveness for training robust gesture recognition systems suitable for real-
            world deployment in challenging operational environments.
          </p>
        </div>
      </div>
    </div>
    <!-- Paper image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-image">
          <img src="./static/images/dataset_overview.png" alt="EgoNRG Dataset Overview" style="width: 100%; max-width: 800px;">
        </div>
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            Overview of the EgoNRG dataset contents. The dataset contains 3,044 videos and 160,639 annotated frames captured from 
            the egocentric perspective. 32 participants performed 12 gestures related to ground vehicle robot control. The dataset features
            conditions with and without background people visible, and both indoor and outdoor environments. The dataset was also recorded from
            four synchronized monochrome cameras each with a different perspective of each gesture performed by the participants.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Dataset Overview</h2>
          <p>
            EgoNRG -Egocentric Navigation Robot Gestures- is a comprehensive dataset features joint hand and arm segmentations captured from 32 participants 
            (14 females and 18 males) performing 12 gesture-based commands for ground vehicle robot control. The 
            participants were divided into four groups of eight, with each group executing a specific set of 
            four gestures. Ten of the twelve gestures were derived from the Army Field Manual, 1 deictic gesture, and 1 emblem gesture. The dataset encompasses 3,044 videos and 160,639 annotated frames. The dataset features:
          </p>
          <ul> 
            <li>Joint hand and arm segmentations of each participants' left and right limb.</li>
            <li>Participants' performed gestures with 1. long sleeves and gloves (wearing replica flame-resistant solid color clothing and military camouflage) and 
            2. bare skin to mimic conditions in real-world industrial and military environments.</li>
            <li>Environments with and without background people visible.</li>
            <li>Data captured in both indoor and outdoor environment at various points throughout the day (morning, midday, and dusk).</li>
            <li>Data captured from four synchronized monochrome cameras each with a different perspective.</li>
            <li>Gesture performed map directly to standard ground vehicle robot commands (stop, move forward, go left, move in reverse, etc.).</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Section Title -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Segmentation Examples</h2>

        </div>
      </div> 
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-image">
          <img src="./static/images/dataset_masks_subset.png" alt="EgoNRG Dataset Gestures" style="width: 100%;">
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
        <p style="margin-top: 20px">
          These are examples of the segmentation masks that were annotated for the EgoNRG dataset.
          You can seee the segmentation masks created for the joint hand-arm for both the left and right limbs.
          You can also see how the dataset has varying clothing conditions, light conditions, and background people visible.
        </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Gesture Classes</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-image">
                <img src="./static/images/gestures_overview_static.png" alt="EgoNRG Dataset Gestures" style="width: 100%;">
              </div>
            </div>
          </div>
          <p style="margin-top: 30px">
            Above is an image depicting a static representation of the 12 gestures performed in the dataset. 10 where adopted from the Army Field Manual, and 
            1 is a deictic gesture "point", and 1 emblem gesture "approve". Below is a video recorded in the third-person view showing an example of each gesture 
            class that was captured in the dataset from the first-person view.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/gif_gesture_examples.mp4"
                    type="video/mp4">
          </video>
          <p style="margin-top: 0px">
            Above is a video showing the 12 gestures being performed from the third-person viewpoint. This is just for reference.
            All gestures in the dataset were captured using the first-person point of view.
          </p>
        </div>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Example Videos</h2>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/example_videos_a_cropped.mp4"
                    type="video/mp4">
          </video>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/example_videos_b_cropped.mp4"
                    type="video/mp4">
          </video>
          <p style="margin-top: 0px">
            Examples of raw videos captured from the various viewpoints of the cameras. You can see gestures being performed
            in both indoor and outdoor environments, with participants wearing long sleeves and gloves as well as bare skin,
            in varying lighting conditions, with and without background people visible.
          </p>
        </div>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Egocentric Viewpoints</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column">
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/combined_video_example_cropped.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
          <p style="margin-top: 0px">
            This video shows all four viewpoints that were captured in the dataset for each gesture performed by the participants.
            You can see that in each viewpoint, the part of the participants hand-arm that is visible is different across viewpoints,
            hence providing more information for training models that can generalize to other egocentric vision platforms.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{regal2026egocentric,
  author    = {Regal, Frank and Nair, Sanat and Karmakar, Asha and Pryor, Mitch},
  title     = {Egocentric Gesture Dataset for Robust Human-Robot Communication via Head Mounted Devices in Industrial and Military Settings},
  journal   = {20th IEEE International Conference on Automatic Face and Gesture Recognition},
  year      = {SUBMITTED - UNDER REVIEW},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            Special thanks to <a href="https://github.com/keunhong">Keunhong Park</a> for developing and open-sourcing this template. 
            Source code for this website can be found at <a href="https://github.com/UTNuclearRoboticsPublic/egonrg.github.io">UTNuclearRoboticsPublic/egonrg.github.io</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
